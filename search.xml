<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[leetcode2]]></title>
    <url>%2F2020%2F05%2F19%2Fleetcode2%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[leetcode经典链表相关题目(思路、方法、code)]]></title>
    <url>%2F2020%2F05%2F18%2Fleetcode1%2F</url>
    <content type="text"><![CDATA[链表是最常用的数据结构之一。 首先是链表的结构定义: 1234567* Definition for singly-linked list.* struct ListNode &#123;* int val;* ListNode *next;* ListNode(int x) : val(x), next(NULL) &#123;&#125;* &#125;;*/ 206.反转链表给定一个链表头节点，将其反转，并输出新的头节点 123456789101112131415161718192021class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; if(head==NULL||head-&gt;next==NULL) return head; else &#123; ListNode* mid=NULL; ListNode* next_node=head-&gt;next; head-&gt;next=NULL;//这一步很重要，防止链表循环 while(next_node!=NULL)&#123; mid=next_node; next_node=next_node-&gt;next; mid-&gt;next=head; head=mid; &#125; return head; &#125; &#125; &#125;; 时间复杂度为O(n), 空间复杂度O(1) 递归法： 1234567891011121314151617class Solution &#123;public: ListNode* reverseList(ListNode* head) &#123; #如果为NULL或单节点则返回自身 if(head==NULL||head-&gt;next==NULL) return head; else &#123; ListNode* newhead=NULL; newhead=reverseList(head-&gt;next); //这意味这已经将除了head之后节点的已经反转 head-&gt;next-&gt;next=head; //故需要令让head-&gt;next的next指向head实现head的反转 head-&gt;next=NULL; //防止链表循环，将head的next置空 //每层递归都要返回最后一个节点 return newhead; &#125; &#125;&#125;; 92. 反转链表 II反转从m到n的链表，请使用一趟扫描完成反转 说明：1$\le$m$\le$n$\le$链表长度 12输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL, m = 2, n = 4输出: 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;NULL 160. 相交链表编写一个程序，找到连个单链表相交的起始节点。 解题思路： 从图中我们可以发现，在相交之后的链表长度是相同的，所以我们从头开始遍历知道两个链表长度相同时再逐个进行比较。 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123;public: ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) &#123; int lenA=0,lenB=0; if (headA==NULL||headB==NULL) return NULL; ListNode* node=headA; while(node!=NULL)&#123; node=node-&gt;next; lenA++; &#125; node=headB; while(node!=NULL)&#123; node=node-&gt;next; lenB++; &#125; int len=lenA-lenB; if(lenA&lt;lenB)&#123; node=headA; headA=headB; headB=node; len*=-1; &#125; while(len&gt;0)&#123; headA=headA-&gt;next; len--; &#125; while(headA!=NULL)&#123; if(headA==headB) return headA; headA=headA-&gt;next; headB=headB-&gt;next; &#125; return NULL; &#125;&#125;; 时间复杂度为O(n)，空间复杂度为O(1)。 141. 环形链表这类链表题目一般都是使用双指针法解决的，例如寻找距离尾部第K个节点、寻找环入口、寻找公共尾部入口等。 给定一个链表，判断链表中是否有环。 第一种结果： fast 指针走过链表末端，说明链表无环，直接返回 null； TIPS: 若有环，两指针一定会相遇。因为每走 11 轮，fast 与 slow 的间距 +1+1，fast 终会追上 slow； 思路：使用快慢指针：快指针每次遍历两个节点，慢指针每次遍历一个节点，没有换很快回NULL，有的话两个指针将会相遇。 1234567891011121314151617181920212223242526272829class Solution &#123;public: bool hasCycle(ListNode *head) &#123; if(head==NULL||head-&gt;next==NULL) return false; ListNode* quick=head; ListNode* slow=head; while(quick!=NULL)&#123; if(quick-&gt;next!=NULL&amp;&amp;quick-&gt;next-&gt;next!=NULL) quick=quick-&gt;next-&gt;next; else quick=NULL; slow=slow-&gt;next; if(quick==slow) return true; &#125; return false; &#125;&#125;;//附上一年前的代码bool hasCycle(struct ListNode *head) &#123; struct ListNode *fast=head, *slow=head; while( slow &amp;&amp; fast &amp;&amp; fast-&gt;next )&#123; fast=fast-&gt;next-&gt;next; slow=slow-&gt;next; if(fast==slow) return true; &#125; return false;&#125; 142. 环形链表 II在141基础上进行改进，给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 方法1：使用SET或者哈希映射 对于每个节点，将其地址通过哈希映射或者直接存入到SET中，然后依次遍历链表，检测新节点是否已经在SET中或者已经被哈希映射。如果遍历结束，则说明没用环。如果遍历过程中发现该地址已经出现在哈希映射表或者SET中，则说明已经遍历过该节点，说明存在环，且该节点为入环节点。 1234567891011121314151617class Solution &#123;public: ListNode *detectCycle(ListNode *head) &#123; set&lt;ListNode *&gt; node_set; //用set存的是ListNode * while(head) &#123; if(node_set.find(head)!=node_set.end()) //说明之前已经出现 &#123; return head; &#125; node_set.insert(head); //将其加入set，继续遍历 head=head-&gt;next; &#125; return NULL; &#125;&#125;; 上面的做法时间和空间效率都不高。 方法2：快慢指针法： 具体解法： 123456789101112131415161718192021class Solution &#123;public: ListNode *detectCycle(ListNode *head) &#123; ListNode* fast=head,*slow=head; while(1)&#123; if (fast==NULL||fast-&gt;next==NULL) return NULL; fast=fast-&gt;next-&gt;next; slow=slow-&gt;next; if (fast==slow) break;&#125; fast=head; while(fast!=slow) fast=fast-&gt;next,slow=slow-&gt;next; return fast; return NULL; &#125;&#125;; 86. 分隔链表给定一个链表和一个特定值 x，对链表进行分隔，使得所有小于 x 的节点都在大于或等于 x 的节点之前。 你应当保留两个分区中每个节点的初始相对位置。 12输入: head = 1-&gt;4-&gt;3-&gt;2-&gt;5-&gt;2, x = 3输出: 1-&gt;2-&gt;2-&gt;4-&gt;3-&gt;5 分析：可以设置两个辅助头结点，一个用来存储小于x的节点的链接，一个用来存储大于等于x的节点的链接。因此在此基础上遍历该链表，如果当前节点的之小于x，则将其加入到小于x的链表列，否则加入到大于等于x的链表列。最终，将两个链表合并即可。 1234567891011121314151617181920212223242526272829class Solution &#123;public: ListNode* partition(ListNode* head, int x) &#123; if(head==NULL||head-&gt;next==NULL) return head; ListNode a(0),b(0); ListNode* less_head=&amp;a; ListNode* more_head=&amp;b; while(head) //遍历链表，将每一项加入到对应的链表列 &#123; if(head-&gt;val&lt;x) &#123; less_head-&gt;next=head; less_head=less_head-&gt;next; &#125; else &#123; more_head-&gt;next=head; more_head=more_head-&gt;next; &#125; head=head-&gt;next; &#125; //将两个链表列进行合并 more_head-&gt;next=NULL; less_head-&gt;next=b.next;//令小于链表的尾指向more.next即可 return a.next; //返回尾小于链表的next即可 &#125;&#125;; 83. 删除排序链表中的重复元素给定一个排序链表，删除所有重复的元素，使得每个元素只出现一次。 12345输入: 1-&gt;1-&gt;2输出: 1-&gt;2输入: 1-&gt;1-&gt;2-&gt;3-&gt;3输出: 1-&gt;2-&gt;3 1234567891011121314151617181920class Solution &#123;public: ListNode* deleteDuplicates(ListNode* head) &#123; if(head==NULL||head-&gt;next==NULL) return head; ListNode*pre =head; ListNode*next=head-&gt;next; while(next!=NULL)&#123; if(pre-&gt;val==next-&gt;val)&#123; next=next-&gt;next; pre-&gt;next=next;//去掉重复的节点 &#125; else&#123; next=next-&gt;next; pre=pre-&gt;next; &#125; &#125; return head; &#125;&#125;; 82. 删除排序链表中的重复元素 II（递归）给定一个排序链表，删除所有含有重复数字的节点，只保留原始链表中 没有重复出现 的数字。在原来的基础上，只要重复出现了的元素全部删除。 头节点有重复的话将会比较麻烦，这里采用递归的思想： 每次对第一个节点进行检测，如果非重复，则将其加入，对后面继续执行。如果重复，则向后遍历直至与其不一致，执行遍历。 12345678910111213141516171819202122class Solution &#123;public:ListNode* deleteDuplication(ListNode *pHead) &#123; if (pHead==NULL || pHead-&gt;next==NULL) &#123; return pHead; &#125; if (pHead-&gt;val==pHead-&gt;next-&gt;val) &#123; // 当前节点是重复节点 ListNode *node = pHead-&gt;next; while (node != NULL &amp;&amp; node-&gt;val == pHead-&gt;val) &#123; // 向后遍历直至与其不一致 node = node-&gt;next; &#125; return deleteDuplication(node); // 从第一个与当前结点不同的结点继续递归 &#125; else &#123; pHead-&gt;next = deleteDuplication(pHead-&gt;next); // 从下一个节点继续递归 return pHead;//保留当前节点 &#125; &#125;&#125;; 19. 删除链表的倒数第N个节点12给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2.当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5. 思路1：删除倒数第n个，即正数第L-n-1个，我们用一个哑节点作为辅助从而简化情况。 1234567891011121314151617181920class Solution &#123;public:ListNode* removeNthFromEnd(ListNode* head, int n) &#123; ListNode dummy=ListNode(0);dummy.next=head;int lenth=0;ListNode first=head;while(first!=NULL)&#123; lenth++; first=first.next;&#125;lenth-=n;first=dummy;while(lenth&gt;0)&#123; lenth--; first=first.next;&#125;first.next=first.next.next;return dummy.next;&#125;&#125;; 思路二：通过一次遍历完成 上述算法可以优化为只使用一次遍历。我们可以使用两个指针而不是一个指针。第一个指针从列表的开头向前移动 n+1 步，而第二个指针将从列表的开头出发。现在，这两个指针被 n 个结点分开。我们通过同时移动两个指针向前来保持这个恒定的间隔，直到第一个指针到达最后一个结点。此时第二个指针将指向从最后一个结点数起的第 nn 个结点。我们重新链接第二个指针所引用的结点的 next 指针指向该结点的下下个结点。 12345678910111213141516class Solution &#123;public:ListNode* removeNthFromEnd(ListNode* head, int n) &#123; ListNode dummy=ListNode(0); dummy.next=head; ListNode fir=dummy; ListNode sec=dummy; for(int i=0;i&lt;=n;i++) fir=fir.next; while(first!=NULL)&#123; fir=fir.next; sec=sec.next; &#125; sec.next=sec.next.next; return dummy.next;&#125;&#125;； 结果要好一点点。 138. 复制带随机指针的链表给定一个链表，每个节点包含一个额外增加的随机指针，该指针可以指向链表中的任何节点或空节点。 要求返回这个链表的 深拷贝。 我们用一个由 n 个节点组成的链表来表示输入/输出中的链表。每个节点用一个 [val, random_index] 表示： val：一个表示 Node.val 的整数。random_index：随机指针指向的节点索引（范围从 0 到 n-1）；如果不指向任何节点，则为 null 。 2. 两数相加给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 123输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 123456789101112131415161718192021222324class Solution &#123;public:ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) &#123; ListNode *head=new ListNode(0); //head是结果 ListNode *p=l1,*q=l2,*cur=head; int carry=0;//作为进位 int x=0,y=0; while(p!=NULL||q!=NULL) //如果其中一个为0，则将其后面节点均视为0即可 &#123; x=(p==NULL)?0:(p-&gt;val); y=(q==NULL)?0:(q-&gt;val); int sum=x+y+carry; //加起来 carry=sum/10; //如果小于10则为0,否则为1 cur-&gt;next=new ListNode(sum%10); cur=cur-&gt;next; if(p!=NULL) p=p-&gt;next; if(q!=NULL) q=q-&gt;next; &#125; if(carry&gt;0) //说明虽然相加完了,但是还是有进位 cur-&gt;next=new ListNode(1); return head-&gt;next; &#125;&#125;; 148. 排序链表如题，将链表排序。 通过快慢指针找到中点，然后用归并排序。 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: ListNode* sortList(ListNode* head) &#123; if (head == NULL || head-&gt;next == NULL) //链表为空或者单节点 return head; ListNode* pmid; ListNode* slow = head; //慢指针 ListNode* fast = head; //快指针 while (fast &amp;&amp; fast-&gt;next) &#123; pmid = slow; slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; &#125; pmid-&gt;next = NULL; return Merge(sortList(head), sortList(slow)); &#125; ListNode* Merge(ListNode* l1, ListNode* l2) //将两个有序链表合并 &#123; ListNode dummy(0); //作为一个哑结点 ListNode *p = &amp;dummy; while (l1 &amp;&amp; l2) &#123; if (l1-&gt;val &lt; l2-&gt;val) &#123; p-&gt;next = l1; l1 = l1-&gt;next; &#125; else &#123; p-&gt;next = l2; l2 = l2-&gt;next; &#125; p = p-&gt;next; &#125; p-&gt;next = (l1 == NULL)? l2 : l1; return dummy.next; //返回哑结点下一个节点 &#125;&#125;;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode 经典栈相关题目(思路、方法、code)]]></title>
    <url>%2F2020%2F05%2F01%2Fleeetcode3%2F</url>
    <content type="text"><![CDATA[刷几条栈相关的题目。 20. 有效的括号给定一个只包括 ‘(’，’)’，’{’，’}’，’[’，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合 左括号必须以正确的顺序闭合 注意空字符串可被认为是有效字符串 123456示例 1:输入: &quot;()&quot;输出: true示例 2:输入: &quot;()[]&#123;&#125;&quot;输出: true 12345678910111213141516171819class Solution &#123;public: bool isValid(string s) &#123; stack&lt;char&gt; st; for(int i=0;i&lt;s.size();i++) &#123; if(s[i]=='('||s[i]=='&#123;'||s[i]=='[') //左括号加入 st.push(s[i]); else if(st.size()&gt;0&amp;&amp;(s[i]==')'&amp;&amp;st.top()=='('||s[i]==']'&amp;&amp;st.top()=='['||s[i]=='&#125;'&amp;&amp;st.top()=='&#123;'))//右括号需要判断能否消除,不能消除即出错 st.pop(); else return false; &#125; if(st.size()==0) return true; return false; &#125;&#125;; 最小栈设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。 1234push(x) —— 将元素 x 推入栈中。pop() —— 删除栈顶的元素。top() —— 获取栈顶元素。getMin() —— 检索栈中的最小元素。 分析：通常的栈结构，push,pop,top均可以满足，但是 getMin()不能满足，能否增添一个变量用来标记最小值？如果有这个变量，则每次push元素我们都可以将元素进行比较，这样就可以O(1)获取最小元素。但是仔细想想，如果将最小元素pop了出来，那么现在的最下元素怎样标记？只能遍历获得，因此最终是O(n)获取最小元素。 故需要转换思路，怎样才能动态地存取最小元素？采用辅助栈。 使用两个栈，一个栈作为正常栈，另一个栈作为辅助栈。辅助栈用来存取当前的最小值，如果最小值更新，则压入新的最小值即可。 使用两个栈，一个栈作为正常栈，另一个栈作为辅助栈。辅助栈用来存取当前的最小值，如果最小值更新，则压入新的最小值即可。 1234567891011121314151617181920212223242526272829入栈 3 | | | || | | ||_3_| |_3_|stack minStack入栈 5 ,5大于minStack的栈顶，故不压入| | | || 5 | | ||_3_| |_3_|stack minStack入栈 2 ，2小于最小值栈的栈顶，故将其入栈| 2 | | || 5 | | 2 ||_3_| |_3_|stack minStack出栈 2，发现2是最小栈的栈顶，故minStack也出栈| | | || 5 | | ||_3_| |_3_|stack minStack出栈 5，发现5不是最小栈的栈顶，因此minStack 不处理| | | || | | ||_3_| |_3_|stack minStack 123456789101112131415161718192021222324252627282930313233343536class MinStack &#123; private: stack&lt;int&gt; data; stack&lt;int&gt; min;public: /** initialize your data structure here. */ MinStack() &#123; &#125; void push(int x) &#123; data.push(x); if(!min.empty()&amp;&amp;x&lt;=min.top()) //最小栈不是空且最小栈栈顶元素小于等于x，则入栈 min.push(x); if(min.empty()) //最小栈为空 min.push(x); &#125; void pop() &#123; if(data.top()==min.top()) //如果要pop的元素等于最小栈栈顶，则将其pop min.pop(); data.pop(); &#125; int top() &#123; return data.top(); //返回data的栈顶即可 &#125; int getMin() &#123; return min.top(); //返回min的栈顶即可 &#125;&#125;; 946. 验证栈序列给定 pushed 和 popped 两个序列，每个序列中的 值都不重复，只有当它们可能是在最初空栈上进行的推入 push 和弹出 pop 操作序列的结果时，返回 true；否则，返回 false。 1234567891011示例 1：输入：pushed = [1,2,3,4,5], popped = [4,5,3,2,1]输出：true解释：我们可以按以下顺序执行：push(1), push(2), push(3), push(4), pop() -&gt; 4,push(5), pop() -&gt; 5, pop() -&gt; 3, pop() -&gt; 2, pop() -&gt; 1示例 2：输入：pushed = [1,2,3,4,5], popped = [4,3,5,1,2]输出：false解释：1 不能在 2 之前弹出。 用栈来模拟这个过程即可。 123456789101112131415161718192021class Solution &#123;public: bool validateStackSequences(vector&lt;int&gt;&amp; pushed, vector&lt;int&gt;&amp; popped) &#123; stack&lt;int&gt; S; int length=pushed.size(); int j=0; for(int i=0;i&lt;length;i++) &#123; S.push(pushed[i]); while(!S.empty()&amp;&amp;S.top()==popped[j]) //每次push后都要将能消除的消除了 &#123; S.pop(); j++; &#125; &#125; if(!S.empty()) return false; return true; &#125;&#125;; 下一个更大元素 I给定两个 没有重复元素 的数组 nums1 和 nums2 ，其中nums1 是 nums2 的子集。找到 nums1 中每个元素在 nums2中的下一个比其大的值。 nums1 中数字 x 的下一个更大元素是指 x 在 nums2 中对应位置的右边的第一个比 x 大的元素。如果不存在，对应位置输出 −1。 1234567示例 1:输入: nums1 = [4,1,2], nums2 = [1,3,4,2].输出: [-1,3,-1]解释: 对于num1中的数字4，你无法在第二个数组中找到下一个更大的数字，因此输出 -1。 对于num1中的数字1，第二个数组中数字1右边的下一个较大数字是 3。 对于num1中的数字2，第二个数组中没有下一个更大的数字，因此输出 -1。 此处忽略先。 84. 柱状图中最大的矩形给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。 求在该柱状图中，能够勾勒出来的矩形的最大面积。 123输入: [2,1,5,6,2,3]输出: 10自己画图看看。 困难，以后再做。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[sql中各种连接的区别]]></title>
    <url>%2F2019%2F09%2F21%2Fjoin%2F</url>
    <content type="text"><![CDATA[上篇文章讲到了外连接，现在简述一下各种连接之间的区别。 自然连接和等值连接就是保留笛卡尔积关系记录中所有匹配的数据记录。根据左右两表的相同列创建一个隐含的join操作，相同列就是两表中列名相同的两列。 外连接：就是在笛卡尔积的记录中，不仅保留所有匹配的记录，而且还会保留部分不匹配的记录。按照保留不匹配条件数据记录来源可以分为左外连接、右外连接和全外连接。 右外连接：除了保存所有匹配的数据记录，还保留右表中未匹配的数据记录。 全外连接：除了保存所有匹配的数据记录，还保留左表和右表中匹配的数据记录。 左外连接：除了保存所有匹配的数据记录，还保留左表中未匹配的数据记录。 具体的问题以后遇到问题了再来这里说一下。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mysql各种连接以及视图]]></title>
    <url>%2F2019%2F09%2F19%2Fsql1%2F</url>
    <content type="text"><![CDATA[这学期刚学数据库，讲讲遇到的难题。 用两个表（a_table、b_table），关联字段a_table.a_id和b_table.b_id 称为连接，现在我来演示一下MySQL的内连接、外连接（ 左(外)连接、右(外)连接、全(外)连接）。 连接建表1234567891011CREATE TABLE `a_table` ( `a_id` int(11) DEFAULT NULL, `a_name` varchar(10) DEFAULT NULL, `a_part` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8CREATE TABLE `b_table` ( `b_id` int(11) DEFAULT NULL, `b_name` varchar(10) DEFAULT NULL, `b_part` varchar(10) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8 内连接 inner join … on …执行语句：select * from a_table a inner join b_table bon a.a_id = b.b_id; 说明返回关联字段相符的记录，及返回交集部分。 左连接（左外连接） left (outer) join… on …执行语句：select * from a_table a left join b_table bon a.a_id = b.b_id; 很直观：left join 是left outer join的简写，它的全称是左外连接，是外连接中的一种。左(外)连接，左表(a_table)的记录将会全部表示出来，而右表(b_table)只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。 同理，右外连接反过来就行了，这里就不再说了。 以上说的都是最基本的情况，仅仅是为了留下最基础的印象，更复杂的情况还请自行解决。 视图背景：关系型数据库中的数据是由一张一张的二维关系表所组成，简单的单表查询只需要遍历一个表，而复杂的多表查询需要将多个表连接起来进行查询任务。对于复杂的查询事件，每次查询都需要编写MySQL代码效率低下。为了解决这个问题，数据库提供了视图（view）功能。 操作指令 代码 创建视图 CREATE VIEW 视图名（ , , , ）AS SELECT ( , , , ) FROM …; 使用视图 和表一样 修改视图 CREATE OR REPLACE VIEW 视图 AS SELECT() FROM() 查看视图 DESC / SHOW FIELDS FROM 视图 总之，视图时虚拟表，不存储数据，而是按照指定的方式进行查询。说了那么多还是不知道视图时干嘛的，我们直接来看看例子。 我们现在希望查询一个用户的多项数据，但是通常不是在一个表里面，所以我们此时把他们内连接起来然后select。 但是有一天我又要查看pid为p003的数据呢，又要写一次查询是吧，那真的是非常浪费时间和体力了，此时我们可以通过视图来实现。 具体流程 我们在场面的SELECT语句前面加上CREAT VIEW 视图名 AS就行了。 下次我们再要用直接用视图就行了。 下面是修改的视图。 下面是查看视图 视图与数据变更 很简单，普通的更新语句，将表product中的数据进行更新，再通过视图检索： 虽然视图时虚拟表，但是我们仍然可以插入数据。通过下图，我们可以看到，跨表插入数据系统反馈报错，提示不能修改超过一个表的数据。 如果在创建视图的时候制定了“WITH CHECK OPTION”，那么更新数据时不能插入或更新不符合视图限制条件的记录。 综上所述，我们需要注意的有两点： 视图不是表，不直接存储数据，是一张虚拟的表； 一般情况下，在创建有条件限制的视图时，加上“WITH CHECK OPTION”命令。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode26、15]]></title>
    <url>%2F2019%2F07%2F08%2Fleetcode26%2F</url>
    <content type="text"><![CDATA[两道双指针问题。 26.删除排序数组中的重复项题目描述 123给定一个排序数组，你需要在原地删除重复出现的元素，使得每个元素只出现一次，返回移除后数组的新长度。不要使用额外的数组空间，你必须在原地修改输入数组并在使用 O(1) 额外空间的条件下完成。 C++ 1234567891011121314151617class Solution &#123;public: int removeDuplicates(vector&lt;int&gt;&amp; nums) &#123; if (nums.size() &lt;= 1) return nums.size(); int lo = 0; int hi = lo + 1; int n = nums.size(); while (hi &lt; n) &#123; while (hi &lt; n &amp;&amp; nums[hi] == nums[lo]) hi++; nums[++lo] = nums[hi]; &#125; return lo; &#125;&#125;; 15. 三数之和（双指针）问题描述 12给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。 思路 因为需要输出所有结果，所以不推荐使用 map 来做 判断 a + b + c = 0，实际上等价于判断 -a = b + c 基本思路：对数组排序后，对每个 a，用首尾双指针进行遍历，具体过程看代码更清晰 去重的方法：排序后，跳过相同的数即可 注意边界条件 C++ 12345678910111213141516171819202122232425262728293031class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; threeSum(vector&lt;int&gt;&amp; nums) &#123; if (nums.size() &lt; 3) return vector&lt;vector&lt;int&gt;&gt;(); // 输入数量小于 3 直接退出 sort(nums.begin(), nums.end()); // 排序 vector&lt;vector&lt;int&gt;&gt; ret; for (int i = 0; i &lt;= nums.size() - 3; i++) &#123; if (i &gt; 0 &amp;&amp; nums[i] == nums[i - 1]) continue; // 跳过第一个数相同的情况 int target = -nums[i]; int lo = i + 1; int hi = nums.size() - 1; while (lo &lt; hi) &#123; if (nums[lo] + nums[hi] &lt; target) lo++; else if (nums[lo] + nums[hi] &gt; target) hi--; else &#123; ret.push_back(&#123; nums[i], nums[lo], nums[hi] &#125;); lo++, hi--; // 不要忘了这双指针都要移动 while (lo &lt; hi &amp;&amp; nums[lo] == nums[lo - 1]) lo++; // 跳过第二个数相同的情况 while (lo &lt; hi &amp;&amp; nums[hi] == nums[hi + 1]) hi--; // 跳过第三个数相同的情况 &#125; &#125; &#125; return ret; &#125;&#125;;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode200.岛屿的个数]]></title>
    <url>%2F2019%2F07%2F07%2Fleetcode200%2F</url>
    <content type="text"><![CDATA[经典的dfs、bfs问题。 问题描述 12345678910111213141516给定一个由 &apos;1&apos;（陆地）和 &apos;0&apos;（水）组成的的二维网格，计算岛屿的数量。一个岛被水包围，并且它是通过水平方向或垂直方向上相邻的陆地连接而成的。你可以假设网格的四个边均被水包围。示例 1: 输入: 11110 11010 11000 00000 输出: 1示例 2: 输入: 11000 11000 00100 00011 输出: 3 思路 经典的 DFS | BFS 问题，搜索连通域的个数 Code: DFS 思路很简单，通过一个dfs找出连通的岛屿然后标志为0，如此以往直至没有1。 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; int n, m;public: int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; // 注意：是 char 不是 int if (!grid.empty()) n = grid.size(); else return 0; if (!grid[0].empty()) m = grid[0].size(); else return 0; int ret = 0; for (int i=0; i&lt;n; i++) for (int j=0; j&lt;m; j++) &#123; if (grid[i][j] != &apos;0&apos;) &#123; ret += 1; dfs(grid, i, j); &#125; &#125; return ret; &#125; void dfs(vector&lt;vector&lt;char&gt;&gt;&amp; grid, int i, int j) &#123; if (i &lt; 0 || i &gt;= n || j &lt; 0 || j &gt;= m ) // 边界判断（递归基） return; if (grid[i][j] == &apos;0&apos;) return; else &#123; grid[i][j] = &apos;0&apos;; // 如果不想修改原数据，可以复制一个 // 4 个方向 dfs；一些问题会扩展成 8 个方向，本质上没有区别 dfs(grid, i+1, j); dfs(grid, i-1, j); dfs(grid, i, j+1); dfs(grid, i, j-1); &#125; &#125;&#125;; Code: BFS 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Solution &#123; int n, m;public: int numIslands(vector&lt;vector&lt;char&gt;&gt;&amp; grid) &#123; if (!grid.empty()) n = grid.size(); else return 0; if (!grid[0].empty()) m = grid[0].size(); else return 0; int ret = 0; for (int i=0; i&lt;n; i++) for (int j=0; j&lt;m; j++) &#123; if (grid[i][j] != '0') &#123; ret += 1; bfs(grid, i, j); &#125; &#125; return ret; &#125; void bfs(vector&lt;vector&lt;char&gt;&gt;&amp; grid, int i, int j) &#123; queue&lt;vector&lt;int&gt; &gt; q; q.push(&#123;i,j&#125;); grid[i][j] = '0'; while (!q.empty()) &#123; i = q.front()[0], j = q.front()[1]; q.pop(); // 当前节点出队 // 当前节点的四周节点依次入队 if (i &gt; 0 &amp;&amp; grid[i-1][j] == '1') &#123; q.push(&#123;i-1,j&#125;); grid[i-1][j] = '0'; &#125; if (i &lt; n-1 &amp;&amp; grid[i+1][j] == '1') &#123; q.push(&#123;i+1,j&#125;); grid[i+1][j] = '0'; &#125; if (j &gt; 0 &amp;&amp; grid[i][j-1] == '1') &#123; q.push(&#123;i,j-1&#125;); grid[i][j-1] = '0'; &#125; if (j &lt; m-1 &amp;&amp; grid[i][j+1] == '1') &#123; q.push(&#123;i,j+1&#125;); grid[i][j+1] = '0'; &#125; &#125; &#125;&#125;; 总结：dfs比较容易实现而且效率更高。]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[leetcode124. 二叉树中的最大路径和（DFS）]]></title>
    <url>%2F2019%2F07%2F06%2Fleetcode124%2F</url>
    <content type="text"><![CDATA[二叉树dfs操作 题目描述 123456789101112给定一个非空二叉树，返回其最大路径和。本题中，路径被定义为一条从树中任意节点出发，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。输入: [1,2,3] 1 / \ 2 3输出: 6 思路 利用一个子函数，求出每个节点最大深度路径和（做法类似求树的深度） 注意，因为节点中的值可能为负数，所以最大深度路径和不一定都会到达叶子 同样，最大深度路径和也可能为负数，此时应该返回 0 接着对每个节点，经过该节点的最大路径和为 1该节点的值 + 左子树的最大深度路径和 + 右子树的最大深度路径和 空树的最大路径和应该为负无穷（作为递归基）； C++ 代码实现： 1234567891011121314151617181920class Solution &#123; const int inf = 0x3f3f3f3f; int ret = -inf; int maxDeepSum(TreeNode* node) &#123; if (node == nullptr) return 0; int l_sum = max(0, maxDeepSum(node-&gt;left)); int r_sum = max(0, maxDeepSum(node-&gt;right)); ret = max(ret, node-&gt;val + l_sum + r_sum); return node-&gt;val + max(l_sum, r_sum); &#125;public: int maxPathSum(TreeNode* root) &#123; maxDeepSum(root); return ret; &#125;&#125;;]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统之内存管理]]></title>
    <url>%2F2019%2F06%2F24%2Fmemo%2F</url>
    <content type="text"><![CDATA[内存管理部分 内存管理逻辑地址：CPU所生成的地址 物理地址：内存单元看到的地址，用户程序绝不会看到的 逻辑地址–&gt;MMU(内存管理单元)–&gt;物理地址 连续内存分配：位每个进程分配一个连续的内存区域 首次适应、最佳适应、最差适应。 外部碎片：总的可用的内存之和可以满足请求，但是不连续 内部碎片：分配但未使用 分页将物理内存分为固定大小的块，称为帧，逻辑内存也划分同样大小的块，称为页 分页是有内部碎片的，因为最后一帧可能用不完 TLB： 有效访问时间：EAT = (1 + e) a + (2 + e)(1 – a) = 2 + e – a 分段程序相关的数据被划分为一个段，有段号+段偏移 以上是分段和分页结合 虚拟内存感觉好多我都在ucore的实验报告中写过还很详细，不想写了，求求你摇了我吧。 两个特点：进程中所有存储器访问都是逻辑地址，这些逻辑地址在运行时被转换为物理地址。 一个进程可以划分为许多块，在执行过程中，这些块不需要连续地位于主存中 两个效果：主存中保留多个进程，进程可以比主存的全部空间还大 按需调页：只有程序需要时才载入页，那些从未访问的页不会调入到物理内存中 一种极端的情况就是所有的页都不在内存中，就开始执行进程，进程会立即出现页错误，并不断地出现页错误直到所有所需的页均在内存中 性能评价： 有效访问时间：参照页 copy on right：ucore做过 页面置换算法：具体在另一篇博客介绍了 FIFO: 算法：总是淘汰最先调入主存的那一页，或者说在主存中驻留时间最长的那一页（常驻的除外）。 理由：最早调入内存的页面，其不再被访问的可能性最大。 最佳替换算法（OPT）： 算法：调入一页而必须淘汰一个旧页时，所淘汰的页应该是以后不再访问的页或距现在最长时间后再访问的页。 特点：不是实际可行的算法，可用来作为衡量各种具体算法的标准，具有理论意义。 最近最少使用算法（LRU）： 算法：淘汰的页面是在最近一段时间里较久未被访问的那页。 原理：根据程序局部性原理，那些刚被使用过的页面，可能马上还要被使用，而在较长时间里未被使用的页面，可能不会马上使用到。 颠簸 新进程–全局置换算法–页错误–调页–大家都出现页错误–设备不多–就绪队列变空–cpu利用率下降–增加多道程序———系统颠簸 频繁的也调度行为称为颠簸，若一个进程在换页上所用的时间多于执行时间，那么这个进程就在颠簸。 通过局部置换算法（或优先置换算法）能够限制系统颠簸。 Global replacement – process selects a replacement frame from the set of all frames; one process can take a frame from another Local replacement – each process selects from only its own set of allocated frames 全局置换算法—不能控制其页错误率，有着更好的吞吐率； 局部置换算法—不能利用其它进程的空闲内存； 局部模型：研究进程实际正在使用多少帧]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统之文件系统]]></title>
    <url>%2F2019%2F06%2F24%2Ffile%2F</url>
    <content type="text"><![CDATA[文件管理部分 文件系统里面常用的分配方法： 连续分配：创建文件时，给文件分配一组连续的块 优点： 简单，支持顺序存取和随机存取，顺序存取速度快 缺点： 外部碎片，预分配 链式分配方法：创建文件时，每块包含指向下一块的指针 优点：提高了磁盘空间利用率，不存则外部碎片的问题，有利于问价插入、删除和扩充 缺点：存取速度慢，适宜顺去存取，不适宜随机存取，链接指针占用一定的空间 索引分配：创建文件时，信息存放在不连续的物理快中，在文件分配表中有一个一级索引。支持顺序和直接访问，最普遍的一种文件分配形式。 优点： 顺序+随机存取，文件动态增长、删除和插入，充分利用外存空间 缺点： 需要访问两次内存、索引表、具体物理快，索引表本身带来了系统开销 空闲空间管理： 位向量：0101010表示 链表：将空闲磁盘块连接起来 组：n个空闲块的地址放在第一个空闲块中，随后一块包含另外n个空闲块地址 计数：参考连续分配 大规模存储结构 寻道时间：磁头定位到磁道所需要的时间 旋转延迟：磁头到达扇区开始位置的时间 传送时间：传送所需要的时间 磁盘调度算法： FCFS：先到先服务 Pickup：搭便车，经过时顺带调度 SSTF：最短寻道时间优先算法，离现在最近的先调度（较为普遍） SCAN：电梯算法，来回（到端点处） C-SCAN：只在去的时候调度 C-LOOK：和C-SCAN的区别是不到端点]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习基础--卷积神经网络CNN]]></title>
    <url>%2F2019%2F06%2F21%2Fcnn%2F</url>
    <content type="text"><![CDATA[卷积神经网络（Convolutional Neural Network）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。 概述CNN具有可学习的权重和偏置常量(biases)的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出是每个分类的分数，普通神经网络里的一些计算技巧到这里依旧适用。 卷积神经网络默认输入是图像，可以让我们把特定的性质编码入网络结构，使我们的前馈函数更加有效率，并减少了大量参数。 人工神经网络神经元神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为权重（weight）。不同的权重和激活函数，则会导致神经网络不同的输出。 激活过程就像神经传输一样，一个激活然后传下去，每个神经元如下： 可以看出，这是 基本wx + b的形式，具体某个参数的含义也很明显了，其中b为偏置bias，可以理解成为更好达到目标而做调整的偏置项。。 举个例子：参照感知器。 激活函数常用的非线性激活函数有sigmoid、tanh、relu等等，sigmoid函数我们前面介绍过了，但是把值域限制在0，1区间有什么好处呢？就是更好的分类。这里我们可以感受到偏置的作用： 神经网络将多个神经元组织在一起就成了神经网络了，下面是一个简单的三层神经网络： 从左至右分别为输入层（输入向量）、隐藏层（是输入层和输出层之间众多神经元和链接组成的各个层面，含有激活函数）、输出层（输出向量）。 同时，每一层都可能由单个或多个神经元组成，每一层的输出将会作为下一层的输入数据。 下面是张量形式的一种表现，偏置项：x0、a0： 层次结构下面是cs231中给出的层次结构的例子： 最左边是数据输入层，对数据做一些处理，比如去均值（把输入数据各个维度都中心化为0，避免数据过多偏差，影响训练效果）、归一化（把所有的数据都归一到同样的范围）、PCA/白化等等。CNN只对训练集做“去均值”这一步。 然后是中间： CONV：卷积计算层，线性乘积求和。 RELU：激活函数的一种。 POOL：池化层，简言之，即取区域平均或最大。 最右边是全连接层。 你应该能想到卷积计算层是CNN的核心。 卷积计算层卷积：对图像（不同的数据窗口数据）和滤波矩阵（因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。 下面Z的形成便是非严格意义上用了滤波器 总之是每个神经元的权重问题 图像上的卷积 左边是图像输入，中间部分就是滤波器filter（带着一组固定权重的神经元），不同的滤波器filter会得到不同的输出数据，比如颜色深浅、轮廓。相当于如果想提取图像的不同特征，则用不同的滤波器filter，提取想要的关于图像的特定信息：颜色深浅或轮廓。 我们很自然的想到，在滤波的过程中有下面几个参数： a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。 b. 步长stride：决定滑动多少步可以到边缘。 c. 填充值zero-padding：在外围边缘补充若干圈0，使能滑倒末尾 下面是一个例子： 左边是输入（7*7*3中，7*7代表图像的像素/长宽，3代表R、G、B 三个颜色通道） 中间部分是两个不同的滤波器 最右边则是两个不同的输出 我们可以看到，每次滤波器都是针对某一局部的数据窗口进行卷积，这就是所谓的CNN中的局部感知机制，因为一下子接受不了那么多信息，但是局部也是有侧重的，比如看美女，对脸、胸、腿是重点关注，所以这3个输入的权重相对较大。 与此同时，数据窗口滑动，导致输入在变化，但中间滤波器Filter w0的权重（即每个神经元连接数据窗口的权重）是固定不变的，这个权重不变即所谓的CNN中的参数（权重）共享机制。 再打个比方，某人环游全世界，所看到的信息在变，但采集信息的双眼不变。btw，不同人的双眼 看同一个局部信息 所感受到的不同，即一千个读者有一千个哈姆雷特，所以不同的滤波器 就像不同的双眼，不同的人有着不同的反馈结果。 激励层上次我们介绍了激活函数sigmoid，但实际梯度下降中，sigmoid容易饱和、造成终止梯度传递，且没有0中心化，我们可以尝试ReLU。 ReLU的优点是收敛快，求梯度简单。 池化pool层​ 前头说了，池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n），很好理解。 参考资料： https://blog.csdn.net/v_JULY_v/article/details/51812459]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[消极学习和积极学习]]></title>
    <url>%2F2019%2F06%2F20%2Flearn%2F</url>
    <content type="text"><![CDATA[上一篇我们介绍了knn，了解了它是一种lazy learning的算法，那么什么是eager learning呢？ 积极学习（Eager Learning） 在进行某种判断（例如，确定一个点的分类或者回归中确定某个点对应的函数值）之前，先利用训练数据进行训练得到一个目标函数，待需要时就只利用训练好的函数进行决策，显然这是一种一劳永逸的方法。 典型的算法： SVM、Find-S算法、候选消除算法、决策树、人工神经网络、贝叶斯方法; 消极学习(Lazy Learning) 这种学习方式指不是根据样本建立一般化的目标函数并确定其参数，而是简单地把训练样本存储起来，直到需要分类新的实例时才分析其与所存储样例的关系，据此确定新实例的目标函数值。也就是说这种学习方式只有到了需要决策时才会利用已有数据进行决策，而在这之前不会经历 Eager Learning所拥有的训练过程。 典型的算法： KNN、局部加权回归、基于案例的推理 比较： Lazy Learning是一个局部的近似，然而虽然不需要训练，它的复杂度还是需要 O(n)，而且要的存储空间比较大 、决策过程比较慢。 Eager Learning考虑到了所有训练样本，说明它是一个全局的近似，虽然它需要耗费训练时间，但它的决策时间基本为0. 总之，不管是哪种学习方法，我的电脑都是跑不动的。:fu:]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习算法之KNN]]></title>
    <url>%2F2019%2F06%2F20%2Fknn%2F</url>
    <content type="text"><![CDATA[k-最近邻是基于实例的学习方法中最基本的算法。 基本描述K最近邻(kNN，k-NearestNeighbor)分类算法，它的工作原理为：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个“邻居”的信息来进行预测。 在回归任务中可使用“平均法”，即将这k个样本的实值输出标记的平均值作为预测结果；还可基于距离远近进行加权平均或加权投票，距离越近的样本权重越大。既是最简单的机器学习算法之一，也是基于实例的学习方法中最基本的，又是最好的文本分类算法之一。 通过上面的描述，我们可以得知算法理论步骤： 1231）算距离：给定测试对象，计算它与训练集中的每个对象的距离2）找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻3）做分类：根据这k个近邻归属的主要类别，来对测试对象分类 算法优缺点先来说说优点： ①简单，易于理解，易于实现，无需参数估计，无需训练;②精度高，对异常值不敏感（个别噪音数据对结果的影响不是很大）;③适合对稀有事件进行分类;④特别适合于多分类问题(multi-modal,对象具有多个类别标签)，KNN要比SVM表现要好 再来说说缺点： ①KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多。②类别评分不是规格化的（不像概率评分）。③输出的可解释性不强，例如决策树的可解释性较强。 ④还有一点也很容易想到，就是如果我们样本很不平衡的时候，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。不过这个时候我们可以通过采取权值的方法改进（比如最近权值高）。 算法实现算法应该很容易实现，下面来看看在kaggle中的手写数字识别中的实现： 首先我们完成将文件转化成向量的函数，方便读取： 123456789101112# convert image to vector def img2vector(filename): rows = 32 cols = 32 imgVector = zeros((1, rows * cols)) fileIn = open(filename) for row in xrange(rows): lineStr = fileIn.readline() for col in xrange(cols): imgVector[0, row * 32 + col] = int(lineStr[col]) return imgVector 然后我们加载整个数据库 12345678910111213141516171819202122232425262728293031323334353637# load dataSet def loadDataSet(): ## step 1: Getting training set print "---Getting training set..." dataSetDir = './' trainingFileList = os.listdir(dataSetDir + 'trainingDigits') # load the training set numSamples = len(trainingFileList) train_x = zeros((numSamples, 1024)) train_y = [] for i in xrange(numSamples): filename = trainingFileList[i] # get train_x train_x[i, :] = img2vector(dataSetDir + 'trainingDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 train_y.append(label) ## step 2: Getting testing set print "---Getting testing set..." testingFileList = os.listdir(dataSetDir + 'testDigits') # load the testing set numSamples = len(testingFileList) test_x = zeros((numSamples, 1024)) test_y = [] for i in xrange(numSamples): filename = testingFileList[i] # get train_x test_x[i, :] = img2vector(dataSetDir + 'testDigits/%s' % filename) # get label from file name such as "1_18.txt" label = int(filename.split('_')[0]) # return 1 test_y.append(label) return train_x, train_y, test_x, test_y 然后就是我们的knn算法了： 123456789101112131415161718192021222324252627282930313233# classify using kNN def kNNClassify(newInput, dataSet, labels, k): numSamples = dataSet.shape[0] # shape[0] stands for the num of row ## step 1:计算距离 # tile(A, reps): Construct an array by repeating A reps times # the following copy numSamples rows for dataSet diff = tile(newInput, (numSamples, 1)) - dataSet # Subtract element-wise squaredDiff = diff ** 2 # squared for the subtract squaredDist = sum(squaredDiff, axis = 1) # sum is performed by row distance = squaredDist ** 0.5 ## step 2: 排序 # argsort() returns the indices that would sort an array in a ascending order sortedDistIndices = argsort(distance) classCount = &#123;&#125; # define a dictionary (can be append element) for i in xrange(k): ## step 3: 选最符合的k个 voteLabel = labels[sortedDistIndices[i]] ## step 4: 统计标签 # when the key voteLabel is not in dictionary classCount, get() # will return 0 classCount[voteLabel] = classCount.get(voteLabel, 0) + 1 ## step 5: 返回频数最大的标签 maxCount = 0 for key, value in classCount.items(): if value &gt; maxCount: maxCount = value maxIndex = key return maxIndex 最后是测试函数： 1234567891011121314151617181920212223# test hand writing class def testHandWritingClass(): ## step 1: load data print "step 1: load data..." train_x, train_y, test_x, test_y = loadDataSet() ## step 2: training... print "step 2: training..." pass ## step 3: testing print "step 3: testing..." numTestSamples = test_x.shape[0] matchCount = 0 for i in xrange(numTestSamples): predict = kNNClassify(test_x[i], train_x, train_y, 3) if predict == test_y[i]: matchCount += 1 accuracy = float(matchCount) / numTestSamples ## step 4: show the result print "step 4: show the result..." print 'The classify accuracy is: %.2f%%' % (accuracy * 100) 由于这种题目的数据都是完整的，而且向量也是有规格的（32*32），也就是说每个测试例子都没有丢失的数据，而且每个参数的值也是在0-1的，所以这里不用进行数据处理阶段，至于要不要做特征工程我也不清楚。。。]]></content>
      <categories>
        <category>机器学习</category>
        <category>kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简单的机器学习算法--k-means]]></title>
    <url>%2F2019%2F06%2F19%2Fk-means%2F</url>
    <content type="text"><![CDATA[下面一种典型的无监督学习算法，主要用于将相似的样本自动归到一个类别中。 算法概述原理：根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果 常用方法：欧式距离法 原理实现：确定常数K，常数K意味着最终的聚类数目，首先随机选定初始点为质心，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，接着，重新计算每个类的质心(即为类中心)，重复这样的过程，直到质心不再改变，最终就确定了每个样本所属的类别以及每个类的质心。 伪代码如下： 12345选择K个点作为初始质心repeat 将每个点指派到最近的质心，形成K个簇 重新计算每个簇的质心until 簇不发生变化或达到最大迭代次数 注意的问题既然是无监督学习，很多情况下，我们并不知道数据的分布情况，所以我们这个时候要怎么确定k值才能达到我们的预期呢？ 这里就涉及到很多的知识了，搞定了这些那可不得了，目前还没有时间，下次。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习入门：数字识别（逻辑回归）]]></title>
    <url>%2F2019%2F06%2F16%2Fdigit-recognizer%2F</url>
    <content type="text"><![CDATA[logistic回归一般用于二分类问题，比如判断一封邮件是否为垃圾邮件，判断照片中的人是男是女，也可以用于多分类问题，比如k类别，就进行k次logistic回归，下面是实现的一个实例。 思想也和简单，做十次逻辑回归，第一次将0作为一类，其他数字为另外一类，如此类推 1.读取内容并返回m*1024形式的样本以及对应的数字 12345678910111213141516171819def loadData(direction): trainfileList=listdir(direction) m=len(trainfileList) dataArray= zeros((m,1024)) labelArray= zeros((m,1)) for i in range(m): returnArray=zeros((1,1024)) #每个txt文件形成的特征向量 filename=trainfileList[i] fr=open('%s/%s' %(direction,filename)) for j in range(32):#32*32转化成1*1024的形式 lineStr=fr.readline() for k in range(32): returnArray[0,32*j+k]=int(lineStr[k]) dataArray[i,:]=returnArray #存储特征向量 filename0=filename.split('.')[0] label=filename0.split('_')[0] labelArray[i]=int(label) #存储类别 return dataArray,labelArray sigmoid(inX)函数 12def sigmoid(inX): return 1.0/(1+exp(-inX)) 然后用梯度下降算法得到回归系数： alpha是步长，maxCycles是迭代步数。 12345678910def gradAscent(dataArray,labelArray,alpha,maxCycles): dataMat=mat(dataArray) #size:m*n labelMat=mat(labelArray) #size:m*1，表示结果 m,n=shape(dataMat)#获取矩阵大小 weigh=ones((n,1)) #权重初始化，多次迭代后得到结果 for i in range(maxCycles): h=sigmoid(dataMat*weigh) error=labelMat-h #size:m*1 weigh=weigh+alpha*dataMat.transpose()*error return weigh 然后我们就得到了权重 最后我们进行分类，我们这里直接用sigmoid函数进行分类。 12345678910111213141516171819def classfy(testdir,weigh): dataArray,labelArray=loadData(testdir) dataMat=mat(dataArray) labelMat=mat(labelArray) h=sigmoid(dataMat*weigh) #size:m*1 m=len(h) error=0.0 for i in range(m): if int(h[i])&gt;0.5: print int(labelMat[i]),'is classfied as: 1' if int(labelMat[i])!=1: error+=1 print 'error' else: print int(labelMat[i]),'is classfied as: 0' if int(labelMat[i])!=0: error+=1 print 'error' print 'error rate is:','%.4f' %(error/m) 然后我们把这些函数整合起来 1234def digitRecognition(trainDir,testDir,alpha=0.07,maxCycles=10): data,label=loadData(trainDir) weigh=gradAscent(data,label,alpha,maxCycles) classfy(testDir,weigh) 总结 可以看出我只对01进行了分类，而且做法太直接了，算是最基础的题目了，数据分析、特征工程直接忽略了，原因很简单，菜，而且时间也有限，不过这种类型的用逻辑回归处理不太好，所以有时间我会学习并复制另外一种解法。]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统之死锁]]></title>
    <url>%2F2019%2F06%2F16%2F%E6%AD%BB%E9%94%81%2F</url>
    <content type="text"><![CDATA[在同步问题中的哲学家就餐问题中，通过信号量我们没有合适的解决方法，因为那会造成死锁，今天我就复习一下死锁。 死锁特征由于竞争资源或者通信关系 两个或更多线程在执行中出现 用于相互等待只能由其他进程引发的事件 互斥：任何时刻只能由一个进程使用资源实例 占有并等待：进程保持至少一个资源别难过正在等待其他进程的资源 非抢占：资源只能在进程使用后资源释放 循环等待：字面意思 资源分配图： 没有死锁是因为P4可能释放资源类型R2的实例 死锁处理方法通常操作系统忽略死锁 由应用进程处理死锁 死锁预防(Deadlock Prevention)：确保死锁的至少一个必要条件不成立 确保系统永远不会进入死锁状态 死锁避免(Deadlock Avoidance) 在使用前进行判断 只允许不会出现死锁的进程请求资源 死锁检测和恢复(Deadlock Detection &amp; Recovery) 在检测到系统进入死锁状态后 进行恢复 死锁预防限制并发进程对资源的请求 使系统在任何时刻都不满足死锁的必要条件，但是会导致资源的利用率低。 互斥 把互斥的共享资源封装成可同时访问 持有并等待 进程请求资源时 要求它不持有任何其他资源 仅允许进程在开始执行时 一次请求所有需要的资源 非抢占 如进程请求不能立即分配的资源 则释放已占有资源 只在能够同时获得所有需要资源时 才执行分配操作 循环等待 对资源排序 要求进程按顺序请求资源 死锁避免构造一个算法以确保系统绝不会进入死锁状态，一直处于安全状态。 安全状态 系统处于安全状（以某种序列分配资源能避免死锁） 针对所有已占有进程存在安全序列 序列&lt;P1, P2, …, PN&gt; 是安全的 Pi要求的资源 &lt;= 当前可用资源 + 所有Pj持有资源 其中 j &lt; i 如Pi的资源请求不能立即分配 则Pi等待所有Pj(j &lt; i) 完成 Pi完成后 Pi + 1 可得到所需资源 执行并释放所分配的资源 最终整个序列的所有Pi都能获得所需资源 资源分配图方法 每个资源有多个实例的时候这种方法就不适用了 只有在将申请边变成分配边而不会导致资源分配图成环时，才安全。 银行家算法 不能满足所有客户的需求时，不分配现金。 当一个进程申请使用资源的时候，银行家算法通过先 试探 分配给该进程资源，然后通过安全性算法判断分配后的系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待。 数据结构实现： 12n = 线程数量m = 资源类型数量 Max(总需求量) = n x m 矩阵 Available(剩余空闲量) 长度为 m 的向量 Allocation(已分配量) = n x m 矩阵 Need(未来需要量) = n x m 矩阵 1Need[i, j] = Max[i, j] - Allocation[i, j] 安全状态判断： 初始化： 12Work = Available // 当前资源剩余空闲量Finish[i] = false for i : 1, 2, ..., n // 线程i有没有完成 寻找线程Ti: Finish[i] = false Need[i] &lt;= Work 去 步骤2 找到线程Ti Work = Work + Allocation[i] Finish[i] = true 回到 步骤1 查所有线程是否满足 Finish[i] == true 若等 则系统处于安全状态 资源分配算法： 1234567891011init: Requesti 线程Ti的资源请求向量 Requesti[j] 线程Ti请求资源Rj的实例do-while:1.如果 Requesti ≤ Need[i], 转到步骤2。否则, 拒绝资源申请, 因为线程已经超过了其最大要求2.如果 Requesti ≤ Available, 转到步骤3。否则, Ti 必须等待, 因为资源不可用3.通过安全状态判断来确定是否分配资源给Ti: 生成一个需要判断状态是否安全的资源分配环境Available = Available - Requesti;Allocation[i] = Allocation[i] + Requesti;Need[i]= Need[i] – Requesti;若安全 则分配资源给Ti若不安全 则拒绝Ti的资源请求 实例：]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统之CPU调度]]></title>
    <url>%2F2019%2F06%2F14%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8BCPU%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[基本概念CPU调度：从就绪队列中选择进程并为之分配CPU 抢占式：进程从运行状态切换到就绪状态、等待-&gt;就绪（优先级高则抢占） 非抢占式：运行-&gt;等待，终止 比较调度算法的准则 CPU利用率 CPU 处于忙状态的 时间百分比 吞吐量 单位时间内完成的 进程数量 周转时间 进程从 初始化 到 结束(包括等待)的总时间 等待时间 进程在 就绪队列中 的总时间 响应时间 从提交请求到产生响应所花费的总时间 调度算法 先到先服务算法（FCFS） 非抢占的，简单，平均等待时间长 短进程优先算法 SPN: Shortest Process Next SJF: Shortest Job First(最短作业优先调度算法) 真正困难时如何知道下一个CPU区间的长度 SRT: Shortest Remaining Time(短剩余时间优先算法) 优先级调度：可以实抢占式和非抢占的 可能会导致无穷阻塞或者饥饿，解决方法是老化 时间片轮转算法 (各个进程轮流占用一个相等的时间片) 进程可能只需要小于时间片的CPU 区间。对于这种情况， 进程本身会自动释放CPU 调度程序接着处理就绪队列的下一个进程。否则，如果当前运 行进程的 CPU 区间比时间片要长，定时器会中断并产生操作系统中断，然后进行上下文切换，将进程加入到就绪队列的尾部，接着CPU 调度程序会选择就绪队列中的下一个进程。 多级队列调度：每个队列有自己的调度算法 多级反馈队列算法 (将就绪队列分成不同子序列 使用不同的算法) 多级反馈队列调度算法 (multilevel feedback queue scheduling algorithm) 允 许进程在队列之间移动。主要思想是根据不同CPU 区间的特点以区分进程。如果进程使用 过多 CPU 时间，那么它会被转移到更低优先级队列。这种方案将νo 约束和交互进程留在 更高优先级队列。此外，在较低优先级队列中等待时间过长的进程会被转移到更高优先级队列。这种形式的老化阻止饥饿的发生。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[操作系统之进程切换]]></title>
    <url>%2F2019%2F06%2F14%2F%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[背景进程是操作系统对一个正在运行的程序的一种抽象。在一个系统上可以同时运行多个进程，而每个进程都好像在独占的使用硬件。在大多数系统中，需要运行的进程数是多于可以运行他们的CPU个数的。无论是在多核还是单核系统中，一个CPU看上去都像是在并发的执行多个进程，这是通过处理器在进程间切换来实现的。 操作系统实现这种交错执行的机制称为上下文切换。，总而言之，进程是轮流使用处理器的。 进程进程：一个执行中的程序的实例，系统中的每个程序都是运行在某个进程的上下文中的。，通常进程还包括堆栈段和数据段，不同的进程文本段相同，其他可能不同。 进程五状态： 创建，运行，等待，就绪，终止 一次只能由一个进程在处理器上运行，但是多个进程可处于就绪或者等待状态 PCB 进程调度多道程序设计的目的就是无论何时都有进程在运行，从而使CPU的利用率最大化 进程调度是选择一个可用的进程 调度队列 作业队列：系统中的所有进程 就绪队列：驻留在内存中的就绪的、等待运行的进程 调度程序 长期（作业）调度程序（不频繁）：从缓冲池中选择进程，装入内存中准备执行。 它看i给你之多道程序设计的程度（内存中的进程数量） 短期（cpu）调度程序（频繁）：从准备执行的进程中选择进程，并为之分配cpu 上下文切换操作系统保持跟踪进程运行所需的所有状态信息，这种状态，也就是上下文，它包括许多信息，例如PC和寄存器文件的当前值，以及主存的内容。 当操作系统决定要把控制权从当前进程转移到某个新进程时，就会进行上下文切换，即保存当前进程的上下文，恢复新进程的上下文，然后将控制权传递到新进程，新进程就会从上次停止的地方开始 上下文切换的过程 保存当前进程的上下文 恢复某个先前被抢占的进程被保存的上下文 将控制传递给这个新恢复的进程 而系统调用进行的是模式切换(mode switch)。模式切换与进程切换比较起来，容易很多，而且节省时间，因为模式切换最主要的任务只是切换进程寄存器上下文的切换。 那么进程切换何时发生呢？进程切换一定发生在中断／异常／系统调用处理过程中，常见的有以下情况： 1、阻塞式系统调用、虚拟地址异常。 导致被中断进程进入等待态。 2、时间片中断、I/O中断后发现更改优先级进程。 导致被中断进程进入就绪态。 3、终止用系统调用、不能继续执行的异常。 导致被中断进程进入终止态。 但是并不意味着所有的中断／异常都会引起进程切换。有一些中断／异常不会引起进程状态转换，不会引起进程切换，只是在处理完成后把控制权交还给被中断进程。 以下是处理流程： 1、（中断／异常等触发）正向模式切换并压入PSW／PC 。 2、保存被中断进程的现场信息。 3、处理具体中断、异常。 4、恢复被中断进程的现场信息。 5、（中断返回指令触发）逆向模式转换并弹出PSW／PC。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kaggle入门：逻辑回归应用之泰坦尼克之灾]]></title>
    <url>%2F2019%2F06%2F11%2Ftitanic%2F</url>
    <content type="text"><![CDATA[上次说了特征工程，这次照着别人的博客来复现一下kaggle上的hello world，python语法之类的我还是不熟悉，所以尽量把第一个项目做好，我这里仅仅是把大体的流程介绍一下，不会涉及到太多的语法。 作者： 寒小阳 出处：http://blog.csdn.net/han_xiaoyang/article/details/49797143 声明：版权所有，转载请注明出处，谢谢。 Github链接：https://github.com/HanXiaoyang/Kaggle_Titanic 题目链接：https://www.kaggle.com/c/titanic 背景泰坦尼克号问题之背景 就是那个大家都熟悉的『Jack and Rose』的故事，豪华游艇倒了，大家都惊恐逃生，可是救生艇的数量有限，无法人人都有，副船长发话了『lady and kid first！』，所以是否获救其实并非随机，而是基于一些背景有rank先后的。 训练和测试数据是一些乘客的个人信息以及存活状况，要尝试根据它生成合适的模型并预测其他人的存活状况。对，这是一个二分类问题，是我们之前讨论的logistic regression所能处理的范畴。 手把手教程马上就来，先来两条我看到的，觉得很重要的经验。 印象中Andrew Ng老师似乎在coursera上说过，应用机器学习，千万不要一上来就试图做到完美，先撸一个baseline的model出来，再进行后续的分析步骤，一步步提高，所谓后续步骤可能包括『分析model现在的状态(欠/过拟合)，分析我们使用的feature的作用大小，进行feature selection，以及我们模型下的bad case和产生的原因』等等。 Kaggle上的大神们，也分享过一些experience，说几条我记得的哈： 『对数据的认识太重要了！』『数据中的特殊点/离群点的分析和处理太重要了！』『特征工程(feature engineering)太重要了！在很多Kaggle的场景下，甚至比model本身还要重要』 『要做模型融合(model ensemble)啊啊啊！』 初探数据123456import pandas as pd #数据分析import numpy as np #科学计算from pandas import Series,DataFramedata_train = pd.read_csv("/Users/Hanxiaoyang/Titanic_data/Train.csv")data_train 得到典型的dataframe格式 读取了数据之后，我们可以了解一下大体的情况 123#各个属性的数目以及百分比data_train.info()data_train.describe() 数据初步分析每个乘客都这么多属性，那我们咋知道哪些属性更有用，而又应该怎么用它们啊？说实话这会儿我也不知道，但我们记得前面提到过 - 『对数据的认识太重要了！』 - 『对数据的认识太重要了！』 - 『对数据的认识太重要了！』 重要的事情说三遍，恩，说完了。仅仅最上面的对数据了解，依旧无法给我们提供想法和思路。我们再深入一点来看看我们的数据，看看每个/多个 属性和最后的Survived之间有着什么样的关系呢。 接下来就是数据分析的内容了，我们分别考察下列情况： 乘客各属性分布（各个属性的人数） 然后是各个属性与获救结果的关联统计（看属性与获救结果是否有关） ps：这个画图真有意思，这个库很不错 简单数据预处理大体数据的情况看了一遍，对感兴趣的属性也有个大概的了解了。 下一步干啥？咱们该处理处理这些数据，为机器学习建模做点准备了。 对了，我这里说的数据预处理，其实就包括了很多Kaggler津津乐道的feature engineering过程，灰常灰常有必要！ 『特征工程(feature engineering)太重要了！』 『特征工程(feature engineering)太重要了！』 『特征工程(feature engineering)太重要了！』 恩，重要的事情说三遍。 先从最突出的数据属性开始吧，对，Cabin和Age，有丢失数据实在是对下一步工作影响太大。 先说Cabin，暂时我们就按照刚才说的，按Cabin有无数据，将这个属性处理成Yes和No两种类型吧。 我们这里用scikit-learn中的RandomForest来拟合一下缺失的年龄数据(注：RandomForest是一个用在原始数据中做不同采样，建立多颗DecisionTree，再进行average等等来降低过拟合现象，提高结果的机器学习算法，我们之后会介绍到) 12345678910111213141516171819202122232425262728293031323334353637from sklearn.ensemble import RandomForestRegressor### 使用 RandomForestClassifier 填补缺失的年龄属性def set_missing_ages(df): # 把已有的数值型特征取出来丢进Random Forest Regressor中 age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']] # 乘客分成已知年龄和未知年龄两部分 known_age = age_df[age_df.Age.notnull()].values unknown_age = age_df[age_df.Age.isnull()].values # y即目标年龄 y = known_age[:, 0] # X即特征属性值 X = known_age[:, 1:] # fit到RandomForestRegressor之中 rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1) rfr.fit(X, y) # 用得到的模型进行未知年龄结果预测 predictedAges = rfr.predict(unknown_age[:, 1::]) # 用得到的预测结果填补原缺失数据 df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges return df, rfrdef set_Cabin_type(df): df.loc[ (df.Cabin.notnull()), 'Cabin' ] = "Yes" df.loc[ (df.Cabin.isnull()), 'Cabin' ] = "No" return dfdata_train, rfr = set_missing_ages(data_train)data_train = set_Cabin_type(data_train) 然后我们就把数据补全了 因为逻辑回归建模时，需要输入的特征都是数值型特征，我们通常会先对类目型的特征因子化。 什么叫做因子化呢？字面意思，把不是数值型的类目属性全都转成0，1的数值属性 1234567891011dummies_Cabin = pd.get_dummies(data_train['Cabin'], prefix= 'Cabin')dummies_Embarked = pd.get_dummies(data_train['Embarked'], prefix= 'Embarked')dummies_Sex = pd.get_dummies(data_train['Sex'], prefix= 'Sex')dummies_Pclass = pd.get_dummies(data_train['Pclass'], prefix= 'Pclass')df = pd.concat([data_train, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)df.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)df.head() 这样，看起来，是不是我们需要的属性值都有了，且它们都是数值型属性呢。 有一种临近结果的宠宠欲动感吧，莫急莫急，我们还得做一些处理，仔细看看Age和Fare两个属性，乘客的数值幅度变化，也忒大了吧！！如果大家了解逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！ (╬▔皿▔)…所以我们先用scikit-learn里面的preprocessing模块对这俩货做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。 1234567import sklearn.preprocessing as preprocessingscaler = preprocessing.StandardScaler()age_scale_param = scaler.fit(df['Age'].values.reshape(-1,1))df['Age_scaled'] = scaler.fit_transform(df['Age'].values.reshape(-1,1), age_scale_param)fare_scale_param = scaler.fit(df['Fare'].values.reshape(-1,1))df['Fare_scaled'] = scaler.fit_transform(df['Fare'].values.reshape(-1,1), fare_scale_param)df.head() 恩，好看多了，万事俱备，只欠建模。马上就要看到成效了，哈哈。 逻辑回归建模我们把需要的feature字段取出来，转成numpy格式，使用scikit-learn中的LogisticRegression建模。 1234567891011121314151617from sklearn import linear_model# 用正则取出我们要的属性值train_df = df.filter(regex='Survived|Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')train_np = train_df.values# y即第0列：Survival结果y = train_np[:, 0]# X即第1列及以后：特征属性值X = train_np[:, 1:]# fit到LogisticRegression之中clf = linear_model.LogisticRegression(solver='liblinear',C=1.0, penalty='l1', tol=1e-6)clf.fit(X, y)clf good，很顺利，我们得到了一个model。 先淡定！淡定！你以为把test.csv直接丢进model里就能拿到结果啊…骚年，图样图森破啊！我们的”test_data”也要做和”train_data”一样的预处理啊！！ 12345678910111213141516171819202122232425data_test = pd.read_csv("/home/kesci/input/titanic8120/test.csv")data_test.loc[ (data_test.Fare.isnull()), 'Fare' ] = 0# 接着我们对test_data做和train_data中一致的特征变换# 首先用同样的RandomForestRegressor模型填上丢失的年龄tmp_df = data_test[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]null_age = tmp_df[data_test.Age.isnull()].values# 根据特征属性X预测年龄并补上X = null_age[:, 1:]predictedAges = rfr.predict(X)data_test.loc[ (data_test.Age.isnull()), 'Age' ] = predictedAges#数据正规化data_test = set_Cabin_type(data_test)dummies_Cabin = pd.get_dummies(data_test['Cabin'], prefix= 'Cabin')dummies_Embarked = pd.get_dummies(data_test['Embarked'], prefix= 'Embarked')dummies_Sex = pd.get_dummies(data_test['Sex'], prefix= 'Sex')dummies_Pclass = pd.get_dummies(data_test['Pclass'], prefix= 'Pclass')df_test = pd.concat([data_test, dummies_Cabin, dummies_Embarked, dummies_Sex, dummies_Pclass], axis=1)df_test.drop(['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)#归一化df_test['Age_scaled'] = scaler.fit_transform(df_test['Age'].values.reshape(-1,1), age_scale_param)df_test['Fare_scaled'] = scaler.fit_transform(df_test['Fare'].values.reshape(-1,1), fare_scale_param)df_test.head() 然后就真的可以丢尽model了~ 1234test = df_test.filter(regex='Age_.*|SibSp|Parch|Fare_.*|Cabin_.*|Embarked_.*|Sex_.*|Pclass_.*')predictions = clf.predict(test)result = pd.DataFrame(&#123;'PassengerId':data_test['PassengerId'].values, 'Survived':predictions.astype(np.int32)&#125;)result.to_csv("/home/kesci/work/savedata/titanic/logistic_regression_predictions.csv", index=False) 结果符合预期，然后去make a submission啦啦啦！ 逻辑回归系统优化亲，你以为结果提交上了，就完事了？ 我不会告诉你，这只是万里长征第一步啊(泪牛满面)！！！这才刚撸完baseline model啊！！！还得优化啊！！！ ……未完持续，等我搞定baseline后再来继续优化 本文中用机器学习解决问题的过程大概如下图所示：]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习初步：逻辑回归]]></title>
    <url>%2F2019%2F06%2F11%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[概念逻辑回归是应用非常广泛的一个分类机器学习算法，它将数据拟合到一个logit函数(或者叫做logistic函数)中，从而能够完成对事件发生的概率进行预测。 由来我们知道，线性回归可以预测出连续值结果，但是在生活中，我们是主要要解决分类问题，这个时候我们可以通过设定一个阈值来解决这个问题，但是这种模型很难对复杂的情况进行的分析。 所以线性回归+阈值并不好，这个时候逻辑回归诞生了！ 核心思想：线性回归的结果输出是一个连续值，而值的范围是无法限定的，那我们有没有办法把这个结果值映射为可以帮助我们判断的结果呢。ok，sigmoid函数可以实现这种功能。 所以我们定义线性回归的预测函数为Y=WTX，那么逻辑回归的输出Y= g(WTX)，其中y=g(z)函数正是上述sigmoid函数(或者简单叫做S形函数)。 判定边界那么为什么通过把函数值限制在一定范围内就可以分类呢，我们还需要判定边界。可以理解为是用以对不同类别的数据分割的边界，边界的两旁应该是不同类别的数据。下面是一个例子： 这个边界是没有规则的，那么逻辑回归是如何根据样本点获得这些边界的呢？ 回到sigmoid函数，我们发现： ​ 当g(z)≥0.5时,，z&gt;=0,意味着y=1，故z=0是一个决策边界 解释： hθ(x)的因变量取值区间是[0，1]，并且关于坐标点（0，0.5）对称（可从Sigmoid Logistic Function图中看出来），所以以hθ(x)=g(θTX)=0.5的点为分界点，所以hθ(x)=g(θTX)≥0.5,时，则θTX≥0。而最后预估y的时候是把y分为两部分，分别用1和0去表示，hθ(x)=g(θTX)≥0.5的点表示为1，hθ(x)=g(θTX)≤0.5的点表示为0. 在逻辑回归中，z就是θTX。 先看第一个例子hθ(x)=g(θ0+θ1X1+θ2X2)，其中θ0 ,θ1 ,θ2分别取-3, 1, 1。则当−3+X1+X2≥0时, y = 1; 则X1+X2=3是一个决策边界，图形表示如下，刚好把图上的两类点区分开来： 至于那些更复杂的曲线，hθ(x)也更复杂，这就是上面那个曲线边界出现的原因 代价函数与梯度下降所以无论二维三维更高维，分类边界可以统一表示成h(x)=ΘTx，我们如何判定hθ(x)中的参数θ是否合适？如何选择θ？ 我们要寻找出最佳参数Θ，使得对于1类别的点x，h(x)趋于正无穷，对于0类别的点x，h(x)趋于负无穷 跳过大量的证明，由cost函数的得知，我们是通过sigmoid函数进行的计算然后找出最佳的θ，从而得出合适的边界，在处理数据时我们通过边界判断。 （这个时候就不是判定边界的事情了，只要你的参数确定了，判定边界自然也就确定了，所以核心行还是参数的确定，这个时候需要用到数分的知识，通过多次迭代，找出代价最小的参数（可能有多个参数12345……）,而我们是通过给定的代价函数求参数的） 所谓的代价函数Cost Function，其实是一种衡量我们在这组参数下预估的结果和实际结果差距的函数，下面是线性回归的代价函数，比较直观： 当然核心是选择合适的参数θ，我们要找到 有曲面为碗状的作为合适的代价函数g(θTx)好像并不符合这种特性， 这个函数有很好的惩罚效果，具体推理会在西瓜上总结 下面我们说说梯度下降，梯度下降算法是调整参数θ使得代价函数J(θ)取得最小值的最基本方法之一。从直观上理解，就是我们在碗状结构的凸函数上取一个初始值，然后挪动这个值一步步靠近最低点的过程 从数学上理解，我们为了找到最小值点，就应该朝着下降速度最快的方向(导函数/偏导方向)迈进，每次迈进一小步，再看看此时的下降最快方向是哪，再朝着这个方向迈进，直至最低点。这个应该不难理解吧。 这个部分唯一的疑点就是我变量的幂要怎么选取（非线性的话要对变量进行映射） 代码与实现只有实践能够解决所有疑问，上面说那么多没啥用的。 下面是python画出的一份数据图 我们来看看关键的代码实现 12345678910111213141516171819202122232425262728#sigmoid函数def sigmoid(X): '''Compute sigmoid function ''' den =1.0+ e **(-1.0* X) gz =1.0/ den return gzdef compute_cost(theta,X,y): '''computes cost given predicted and actual values''' m = X.shape[0]#number of training examples theta = reshape(theta,(len(theta),1)) J =(1./m)*(-transpose(y).dot(log(sigmoid(X.dot(theta))))- transpose(1-y).dot(log(1-sigmoid(X.dot(theta))))) grad = transpose((1./m)*transpose(sigmoid(X.dot(theta))- y).dot(X)) #optimize.fmin expects a single value, so cannot return grad return J[0][0]#,graddef compute_grad(theta, X, y): '''compute gradient''' theta.shape =(1,3) grad = zeros(3) h = sigmoid(X.dot(theta.T)) delta = h - y l = grad.size for i in range(l): sumdelta = delta.T.dot(X[:, i]) grad[i]=(1.0/ m)* sumdelta *-1 theta.shape =(3,) return grad 总结它始于输出结果为有实际意义的连续值的线性回归，但是线性回归对于分类的问题没有办法准确而又具备鲁棒性地分割，因此我们设计出了逻辑回归这样一个算法，它的输出结果表征了某个样本属于某类别的概率。 而直观地在二维空间理解逻辑回归，是sigmoid函数的特性，使得判定的阈值能够映射为平面的一条判定边界，当然随着特征的复杂化，判定边界可能是多种多样的样貌，但是它能够较好地把两类样本点分隔开，解决分类问题。 求解逻辑回归参数的传统方法是梯度下降，构造为凸函数的代价函数后，每次沿着偏导方向(下降速度最快方向)迈进一小部分，直至N次迭代后到达最低点。 总结一下思绪，logistic回归的任务就是要找到最佳的拟合参数,从而得出边界，从而可以对数据进行判断。。 不愧是逻辑回顾，先撤了，具体的实现过一阵子看看，现在会用逻辑回归就行了。 又回来看了一下，才发现sigmoid函数的精妙之处，希望有时间能将数学证明过一遍。 参考资料：http://blog.csdn.net/han_xiaoyang/article/details/49123419]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[进程同步]]></title>
    <url>%2F2019%2F06%2F10%2F%E8%BF%9B%E7%A8%8B%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[背景：生产者-消费者问题：并发访和操作同一个数据时的执行结果与访问发生的顺序有关，而进程的同步对操作系统至关重要。 进程并发执行的好处进程需要与计算机中其他进程或设备进行协作 共享资源 加速 I/O操作和CPU计算可以并行 程序可划分成多个模块放在多个处理机上并行执行 模块化 将大程序分解成小程序(gcc会调用 cpp cc1 cc2 as ld) 使系统易于复用和扩展 进程的交互关系 互斥(Mutual Exclusion) 一个进程占用资源 其他进程不能使用 死锁(Deadlock) 多个进程各占用部分资源 形成循环等待 饥饿(Starvation) 其他进程可能轮流占用资源 一个进程一直得不到资源 临界区临界区问题：字面意思 临界区：当一个进程进入临界区时，没有其他进程可以在临界区内执行。 满足的三项要求： 互斥：不能同时在临界区执行 前进：一定时间内选择进入临界区的进程 有限等待：不能等太久 处理临界区问题的两种方法：字面意思 抢占内核 非抢占内核 软件实现Peterson 算法：两个进程在临界区与剩余区间交替执行 硬件同步：锁一个进程进入临界区之前必须得到锁，退出后释放锁 对于单处理器环境，只需要在修改共享变量时禁止中断就行了（非抢占式） 多处理器环境，这种做法会降低效率的，解决方法时原子操作，这就需要硬件上的支持了（锁） 信号量以上都是基础知识，重点是信号量 信号量的用途很广，比较简单的就是司机乘务员之类的，通过信号量解决各种同步问题（比如实现顺序的先后）。 可以用二进制信号量来解决临界区问题（0和1之间的） 有了信号量之后临界区问题变得简单得多，现在来说说信号量的实现 实现 忙等待：看程序可以知道为进入临界区的进程一直在循环之中，这是多道程序不允许的，这样做会浪费cpu时间 自旋锁：会处于忙等待的信号量 克服忙等：修改PV操作呗 优点：节省cpu时间，提高cpu的利用率，让cpu被其他进程有效的利用 缺点：行下文切换花费时间，如果锁占用时间短，那么会得不偿失 实现：在wait操作时，如果要等待，阻塞自己，这个操作将进程放入与信号量相关的等待队列中，并将进程的状态切换为等待状态，然后指向cpu调度程序，选择另一个进程来执行。 当执行signal（）操作之后，通过wakeup操作重新执行该进程，即将状态转换为就绪状态，然后该进程放到就绪队列之中 （S是信号量） 注意block和wakeup之间的对应关系 说了那么多，信号量的关键之处在于他们是原子地执行，这个就是硬件的事儿了 死锁：第七章内容 饥饿：进程在信号量内无限期阻塞（栈） 说了那么多还是同步的问题 好像对于临界区地实现没有过多的关注 线面介绍几个经典的同步问题 经典同步问题有限缓冲问题：生产者-消费者问题 读者-写者问题：字面意思 哲学家就餐问题： 信号量不能很好的解决这个问题，第七章死锁会讲到 管程：略 总结实现同步互斥需要解决临界区问题 临界区一般是共享变量地操作 信号量可以实现同步互斥 进程互斥进程互斥：在多个程序中，有两个进程不可以同时进行（例如读，写操作）。 竞争资源（临界资源） 当并发进程竞争使用同一资源时，他们之间就会发生冲突。如果操作系统将资源分配给其中的某一个进程使用，另一个进程就必须等待，直到申请的资源可用时，由操作系统分配给他们。 如果竞争资源的进程太多，这些进程还必须等待在一个队列中，如就绪队列，阻塞队列等。 一种极端的情况是，被阻塞进程永远得不到申请的资源，而死锁。 采用互斥方式，使用临界资源 资源的互斥，进程使用上述这类资源的时候，只能有一个进程对资源进行处理。（临界区） 进程同步 进程同步是一个操作系统级别的概念,是在多道程序的环境下，存在着不同的制约关系，为了协调这种互相制约的关系，实现资源共享和进程协作，从而避免进程之间的冲突，引入了进程同步。比如说进程A需要从缓冲区读取进程B产生的信息，当缓冲区为空时，进程B因为读取不到信息而被阻塞。而当进程A产生信息放入缓冲区时，进程B才会被唤醒。 同步互斥解决方法（同样是实现临界区地方法）：软件、硬件、信号量、管程]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机器学习之特征工程]]></title>
    <url>%2F2019%2F06%2F09%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[这只是一个很简单的介绍，并没有涉及到很深的知识。 1.特征工程简介数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。 特征处理是特征工程的核心部分，sklearn提供了较为完整的特征处理方法，包括数据预处理，特征选择，降维等。 2.数据预处理初始化的数据可能有各种各样的问题，所以我们要对数据进行数据预处理 不属于同一量纲：即特征的规格不一样，不能够放在一起比较。 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。参考：通常使用哑编码的方式将定性特征转换为定量特征 存在缺失值 信息利用率低 2.0 数据清洗1.基于统计的异常点检测算法 (1).简单统计分析： 比如对属性值进行一个描述性的统计，从而查看哪些值是不合理的，比如针对年龄来说，我们规定范围维 [0,100]，则不在这个范围的样本，则就认为是异常样本 (2).3δ原则（δ为方差）: 当数据服从正态分布：根据正态分布的定义可知，距离平均值3δ之外的概率为 P(|x-μ|&gt;3δ) &lt;= 0.003 ，这属于极小概率事件，在默认情况下我们可以认定，距离超过平均值3δ的样本是不存在的。 因此，当样本距离平均值大于3δ，则认定该样本为异常值 (3).通过极差和四分位数间距，进行异常数据的检测 2.基于距离的异常点检测算法（其实和K近邻算法的思想一样） 主要通过距离方法来检测异常点，将一个数据点与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离(曼哈顿距离)、欧氏距离和马氏距离等方法 3.基于密度的异常点检测算法 考察当前点周围密度，可以发现局部异常点 以上是临时补上去的 2.1无量纲化顾名思义，我们处理数据要把他们转换成统一规格。常见的无量纲化方法有标准化和区间缩放法。标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 2.1.1标准化很简单，一个函数的事 1StandardScaler().fit_transform() 2.1.2 区间缩放法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，应该很好理解。 1MinMaxScaler().fit_transform(iris.data) 2.1.3标准化和归一化的区别标准化是依照特征矩阵的列处理数据，将样本的特征值转换到同一量纲下。 归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”，看下面的公式就行。 1Normalizer().fit_transform(iris.data) 2.2 对定量特征二值化很简单，设定一个阈值，然后： 1Binarizer(threshold=3).fit_transform(iris.data) 2.3 对定性特征哑编码啥名词。。。 由于IRIS数据集的特征皆为定量特征，故使用其目标值进行哑编码（实际上是不需要的）。 1OneHotEncoder().fit_transform(iris.target.reshape((-1,1))) 2.4 缺失值计算终于来了个简单的了，这个看库就可以了，意思一下 1Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) 2.5 数据变换 常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的，下面是一个例子，说简单一点就是搞多一点数据。 1PolynomialFeatures().fit_transform(iris.data) 3.特征选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征，关键是选择特征啊： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 下面介绍三种特征选择方法： 3.1 Filter3.1.1方差选择法很好理解，使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。 1VarianceThreshold(threshold=3).fit_transform(iris.data) 3.1.2相关系数法： 这个不太懂 3.1.3卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。这个涉及到概率论的内容啊，忘得差不多了。 3.1.4 互信息法 信息论知识，经典的互信息也是评价定性自变量对定性因变量的相关性的，选取互信息最大的几个特征就行了 1\#选择K个最好的特征，返回特征选择后的数据 SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) 3.2 Wrapper3.2.1 递归特征消除法递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 3.3 Embedded3.3.1 基于惩罚项的特征选择法 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。直接上函数： 1SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target) 这里等我以后遇到的时候再来写吧，有点麻烦 3.3.2 基于树模型的特征选择法4.降维当特征选择完成后，可以直接训练模型了，电脑太菜只能降维减少训练时间。 常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。 PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。 这两种方法就两行代码，没什么参考意义 5.总结这只是一个粗略的知识点介绍而已，在实际操作中是没有那么简单的，当然要达到你满意的结果需要很长时间的，这种事情以后慢慢说吧。话说还是掘金权威，知乎上的纯属科普。 参考来源： 作者：城东 链接：https://www.zhihu.com/question/29316149/answer/110159647]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[几种常见的内存分配算法总结]]></title>
    <url>%2F2019%2F06%2F02%2Fallcoagri%2F</url>
    <content type="text"><![CDATA[最近在复习操作系统，所以顺带总结一下常见的内存分配算法。 first-fit算法： 该算法从空闲分区链首开始查找，直至找到一个能满足其大小要求的空闲分区为止。然后再按 照作业的大小，从该分区中划出一块内存分配给请求者，余下的空闲分区仍留在空闲分区链 中。 优点：从这个分配算法的实现我们可以看出，这个算法倾向于使用内存中低地址部分的空闲块，因此较高地址的空闲块很少用到这样就会导致高地址部分有很大部分的空闲区，这样做显然位移后有大作业分配空间创造了极佳的条件！ 缺点：从优点中我们也可以看出，低地址被不断地划分，由此会留下很多的难以利用的、很小的空闲区，从而很多的内部碎片，然而每次查找都会从低地址部分开始查找，从而会增加查找的开销！ best-fit算法： 该算法总是把既能满足要求，又是最小的空闲分区分配给作业。算法执行过程中，将所有的空闲区按其大小排序后，以递增顺序形成一个空白链。这样每次找到的第一个满足要求的空闲区，必然是最优的。和其他分配算法对比，该算法似乎是最优的，但事实上并不一定。因为每次分配后剩余的空间一定是最小的，在存储器中将留下许多难以利用的小空闲区。同时每次分配后必须重新排序，这也带来了一定的开销。 优点: 每次分配给作业都是最适合该作业大小的空闲块 缺点：和首次适配类似，在内存中留下了很多难以利用的小的空闲块 worst-fit算法： 该算法按大小递减的顺序形成空闲区链，分配时直接从空闲区链的第一个空闲区中分配（不能满足需要则不分配）。很显然，如果第一个空闲分区不能满足，那么再没有空闲分区能满足需要。这种分配方法初看起来不太合理，但它也有很强的直观吸引力：在大空闲区中放入程序后，剩下的空闲区常常也很大，于是还能装下一个较大的新程序。最坏适应算法与最佳适应算法的排序正好相反，它的队列指针总是指向最大的空闲区，在进行分配时，总是从最大的空闲区开始查寻。该算法克服了最佳适应算法留下的许多小的碎片的不足，但保留大的空闲区的可能性减小了，而且空闲区回收也和最佳适应算法一样复杂。 优点：给文件分配后剩下的空闲区不会太小，对中小型作业分配分区有利 缺点：从算法实现我们可以看出，它使存储器缺乏大的空闲区，对大型文件的分配不利！ buddy算法： Buddy System算法把系统中的可用存储空间划分为存储块(Block)来进行管理, 每个存储块的大小必须是2的n次幂(Pow(2, n)), 即1, 2, 4, 8, 16, 32, 64, 128… 初步理解 我们在练习一中用到的物理内存管理，它的空闲块大小是没有规则的，你想怎么分就怎么分（当然这取决于你的物理内存），而且是通过双向链表键他们链接起来，当要查找合适的空闲块时只能按物理地址顺序线性查找，时间复杂度达到了O（N），现在我们有更加好的物理内存管理方法，能使时间复杂度达到O（log N），这涉及到在上学期数据结构学的二叉树的概念，而且为了更好的进行物理内存管理，规定每个存储块的大小必须是2的n次幂。 优点：快速搜索合并，低外部碎片，，相对于前三种算法来说算是一种比较理想的分配算法 缺点：内部碎片（对于2^n+1等类似大小的会产生很大的内部碎片） 下面是一个内存大小为16的例子： slab算法： 前面几种算法，要么是比较基础，要么是前面介绍过了，就不仔细展开了，现在我主要来介绍一下slab算法： 背景： 内核管理页面使用了2个算法：伙伴算法和slub算法，伙伴算法以页为单位管理内存，但在大多数情况下，程序需要的并不是一整页，而是几个、几十个字节的小内存。于是需要另外一套系统来完成对小内存的管理，这就是slub系统。slub系统运行在伙伴系统之上，为内核提供小内存管理的功能 实现： slub把内存分组管理，每个组分别包含2^3、2^4、…2^11个字节，在4K页大小的默认情况下，另外还有两个特殊的组，分别是96B和192B，共11组。之所以这样分配是因为如果申请2^12B大小的内存，就可以使用伙伴系统提供的接口直接申请一个完整的页面即可。slub就相当于零售商，它向伙伴系统“批发”内存，然后在零售出去。 下面是slab一个示意图： 我们可以看出，slab管理系统对于占用内存较小的请求是很友好的，可以做到任意大小的内存分配！ 优点： 可以提供小块内存的分配支持 不必每次申请释放都和伙伴系统打交道，提供了分配释放效率 如果在slab缓存的话，其在CPU高速缓存的概率也会较高。 伙伴系统的操作队系统的数据和指令高速缓存有影响，slab分配器降低了这种副作用 伙伴系统分配的页地址都页的倍数，这对CPU的高速缓存的利用有负面影响，页首地址对齐在页面大小上使得如果每次都将数据存放到从伙伴系统分配的页开始的位置会使得高速缓存的有的行被过度使用，而有的行几乎从不被使用。slab分配器通过着色使得slab对象能够均匀的使用高速缓存，提高高速缓存的利用率 缺点： 对于微型嵌入式系统，它显得比较复杂，这是可以使用经过优化的slob分配器，它使用内存块链表，并使用最先适配算法 对于具有大量内存的大型系统，仅仅建立slab分配器的数据结构就需要大量内存，这时候可以使用经过优化的slub分配器]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
  </entry>
</search>
